{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5793796,"sourceType":"datasetVersion","datasetId":199387}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Traffic Accident Analysis","metadata":{}},{"cell_type":"markdown","source":"## Data Loading & Understanding","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T09:41:39.594256Z","iopub.execute_input":"2026-01-01T09:41:39.594696Z","iopub.status.idle":"2026-01-01T09:41:39.604933Z","shell.execute_reply.started":"2026-01-01T09:41:39.594656Z","shell.execute_reply":"2026-01-01T09:41:39.603917Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nimport logging\n\nspark = SparkSession.builder \\\n    .appName(\"US Accidents\") \\\n    .getOrCreate()\nspark.sparkContext.setLogLevel(\"ERROR\")\n\nlogging.getLogger(\"py4j\").setLevel(logging.ERROR)\nlogging.getLogger(\"pyspark\").setLevel(logging.ERROR)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"file_path = \"/kaggle/input/us-accidents/US_Accidents_March23.csv\"\n\ndf_spark = spark.read.csv(\n    file_path,\n    header=True,          \n    inferSchema=True,     \n    multiLine=True,       \n    escape='\"'            \n)\n\nprint(\"First 5 records:\")\ndf_spark.show(5, truncate=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T10:29:01.607651Z","iopub.execute_input":"2025-12-30T10:29:01.607999Z","iopub.status.idle":"2025-12-30T10:31:08.595754Z","shell.execute_reply.started":"2025-12-30T10:29:01.607972Z","shell.execute_reply":"2025-12-30T10:31:08.594126Z"}},"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"First 5 records:\n+---+-------+--------+-------------------+-------------------+-----------------+------------------+-------+-------+------------+-------------------------------------------------------------------------------------+-------------------------+------------+----------+-----+----------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+\n|ID |Source |Severity|Start_Time         |End_Time           |Start_Lat        |Start_Lng         |End_Lat|End_Lng|Distance(mi)|Description                                                                          |Street                   |City        |County    |State|Zipcode   |Country|Timezone  |Airport_Code|Weather_Timestamp  |Temperature(F)|Wind_Chill(F)|Humidity(%)|Pressure(in)|Visibility(mi)|Wind_Direction|Wind_Speed(mph)|Precipitation(in)|Weather_Condition|Amenity|Bump |Crossing|Give_Way|Junction|No_Exit|Railway|Roundabout|Station|Stop |Traffic_Calming|Traffic_Signal|Turning_Loop|Sunrise_Sunset|Civil_Twilight|Nautical_Twilight|Astronomical_Twilight|\n+---+-------+--------+-------------------+-------------------+-----------------+------------------+-------+-------+------------+-------------------------------------------------------------------------------------+-------------------------+------------+----------+-----+----------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+\n|A-1|Source2|3       |2016-02-08 05:46:00|2016-02-08 11:00:00|39.865147        |-84.058723        |NULL   |NULL   |0.01        |Right lane blocked due to accident on I-70 Eastbound at Exit 41 OH-235 State Route 4.|I-70 E                   |Dayton      |Montgomery|OH   |45424     |US     |US/Eastern|KFFO        |2016-02-08 05:58:00|36.9          |NULL         |91.0       |29.68       |10.0          |Calm          |NULL           |0.02             |Light Rain       |false  |false|false   |false   |false   |false  |false  |false     |false  |false|false          |false         |false       |Night         |Night         |Night            |Night                |\n|A-2|Source2|2       |2016-02-08 06:07:59|2016-02-08 06:37:59|39.92805900000001|-82.831184        |NULL   |NULL   |0.01        |Accident on Brice Rd at Tussing Rd. Expect delays.                                   |Brice Rd                 |Reynoldsburg|Franklin  |OH   |43068-3402|US     |US/Eastern|KCMH        |2016-02-08 05:51:00|37.9          |NULL         |100.0      |29.65       |10.0          |Calm          |NULL           |0.0              |Light Rain       |false  |false|false   |false   |false   |false  |false  |false     |false  |false|false          |false         |false       |Night         |Night         |Night            |Day                  |\n|A-3|Source2|2       |2016-02-08 06:49:27|2016-02-08 07:19:27|39.063148        |-84.032608        |NULL   |NULL   |0.01        |Accident on OH-32 State Route 32 Westbound at Dela Palma Rd. Expect delays.          |State Route 32           |Williamsburg|Clermont  |OH   |45176     |US     |US/Eastern|KI69        |2016-02-08 06:56:00|36.0          |33.3         |100.0      |29.67       |10.0          |SW            |3.5            |NULL             |Overcast         |false  |false|false   |false   |false   |false  |false  |false     |false  |false|false          |true          |false       |Night         |Night         |Day              |Day                  |\n|A-4|Source2|3       |2016-02-08 07:23:34|2016-02-08 07:53:34|39.747753        |-84.20558199999998|NULL   |NULL   |0.01        |Accident on I-75 Southbound at Exits 52 52B US-35. Expect delays.                    |I-75 S                   |Dayton      |Montgomery|OH   |45417     |US     |US/Eastern|KDAY        |2016-02-08 07:38:00|35.1          |31.0         |96.0       |29.64       |9.0           |SW            |4.6            |NULL             |Mostly Cloudy    |false  |false|false   |false   |false   |false  |false  |false     |false  |false|false          |false         |false       |Night         |Day           |Day              |Day                  |\n|A-5|Source2|2       |2016-02-08 07:39:07|2016-02-08 08:09:07|39.627781        |-84.188354        |NULL   |NULL   |0.01        |Accident on McEwen Rd at OH-725 Miamisburg Centerville Rd. Expect delays.            |Miamisburg Centerville Rd|Dayton      |Montgomery|OH   |45459     |US     |US/Eastern|KMGY        |2016-02-08 07:53:00|36.0          |33.3         |89.0       |29.65       |6.0           |SW            |3.5            |NULL             |Mostly Cloudy    |false  |false|false   |false   |false   |false  |false  |false     |false  |false|false          |true          |false       |Day           |Day           |Day              |Day                  |\n+---+-------+--------+-------------------+-------------------+-----------------+------------------+-------+-------+------------+-------------------------------------------------------------------------------------+-------------------------+------------+----------+-----+----------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+\nonly showing top 5 rows\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"df_spark.printSchema()\nprint(\"Rows:\", df_spark.count())\nprint(\"Columns:\", len(df_spark.columns))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T10:04:55.382262Z","iopub.execute_input":"2025-12-30T10:04:55.382612Z","iopub.status.idle":"2025-12-30T10:05:22.791332Z","shell.execute_reply.started":"2025-12-30T10:04:55.382576Z","shell.execute_reply":"2025-12-30T10:05:22.790580Z"}},"outputs":[{"name":"stdout","text":"root\n |-- ID: string (nullable = true)\n |-- Source: string (nullable = true)\n |-- Severity: integer (nullable = true)\n |-- Start_Time: timestamp (nullable = true)\n |-- End_Time: timestamp (nullable = true)\n |-- Start_Lat: double (nullable = true)\n |-- Start_Lng: double (nullable = true)\n |-- End_Lat: double (nullable = true)\n |-- End_Lng: double (nullable = true)\n |-- Distance(mi): double (nullable = true)\n |-- Description: string (nullable = true)\n |-- Street: string (nullable = true)\n |-- City: string (nullable = true)\n |-- County: string (nullable = true)\n |-- State: string (nullable = true)\n |-- Zipcode: string (nullable = true)\n |-- Country: string (nullable = true)\n |-- Timezone: string (nullable = true)\n |-- Airport_Code: string (nullable = true)\n |-- Weather_Timestamp: timestamp (nullable = true)\n |-- Temperature(F): double (nullable = true)\n |-- Wind_Chill(F): double (nullable = true)\n |-- Humidity(%): double (nullable = true)\n |-- Pressure(in): double (nullable = true)\n |-- Visibility(mi): double (nullable = true)\n |-- Wind_Direction: string (nullable = true)\n |-- Wind_Speed(mph): double (nullable = true)\n |-- Precipitation(in): double (nullable = true)\n |-- Weather_Condition: string (nullable = true)\n |-- Amenity: boolean (nullable = true)\n |-- Bump: boolean (nullable = true)\n |-- Crossing: boolean (nullable = true)\n |-- Give_Way: boolean (nullable = true)\n |-- Junction: boolean (nullable = true)\n |-- No_Exit: boolean (nullable = true)\n |-- Railway: boolean (nullable = true)\n |-- Roundabout: boolean (nullable = true)\n |-- Station: boolean (nullable = true)\n |-- Stop: boolean (nullable = true)\n |-- Traffic_Calming: boolean (nullable = true)\n |-- Traffic_Signal: boolean (nullable = true)\n |-- Turning_Loop: boolean (nullable = true)\n |-- Sunrise_Sunset: string (nullable = true)\n |-- Civil_Twilight: string (nullable = true)\n |-- Nautical_Twilight: string (nullable = true)\n |-- Astronomical_Twilight: string (nullable = true)\n\n","output_type":"stream"},{"name":"stderr","text":"[Stage 3:>                                                          (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"Rows: 7728394\nColumns: 46\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Sampling\n## Choosing a Statistically-Justified Sample Size \\( n \\)\n\nTo select a random sample **without bias** and with **statistical justification**, we use **Cochran’s sample size formula**.  \nThis approach is widely used when the population is large and the true variability is unknown.\n\n\n### A) Cochran’s Formula (Conservative & Widely Used)\n\nWhen estimating a **population proportion**, or when no prior information about variability is available, Cochran’s formula provides a *safe* (worst-case) sample size.\n\n#### Step 1: Initial sample size (infinite population)\n\n$$\nn_0 = \\frac{z^2 \\, p(1 - p)}{e^2}\n$$\n\n\n\n#### Step 2: Finite Population Correction (FPC)\n\nSince the population size \\( N \\) is finite, we adjust \\( $n_0$ \\) as follows:\n\n$$\nn = \\frac{n_0}{1 + \\dfrac{n_0 - 1}{N}}\n$$\n\n\n\n### Definition of Parameters\n\n- \\( z \\): z-score corresponding to the confidence level  \n  - 90% → \\( z = 1.645 \\)  \n  - 95% → \\( z = 1.96 \\)  \n  - 99% → \\( z = 2.576 \\)\n\n- \\( e \\): margin of error  \n  - Example: \\( e = 0.01 \\) corresponds to **1% error**\n\n- \\( p \\): expected population proportion  \n  - If unknown, use  \n    $\n    p = 0.5\n    $\n    This represents the **worst-case scenario** and produces the **largest (safest) sample size**\n\n- \\( N \\): total population size\n\n\n### Interpretation\n\n- Larger confidence level → larger sample size  \n- Smaller margin of error → larger sample size  \n- Using \\( p = 0.5 \\) guarantees conservative, unbiased estimation  \n\nThis method ensures the selected sample is **random**, **representative**, and **statistically defensible**.\n\n\n","metadata":{}},{"cell_type":"code","source":"import math\n\nN = 7728394\nz = 2.576      # 99% confidence\np = 0.5       # conservative\ne = 0.01      # 1% margin of error\n\nn0 = (z**2 * p * (1-p)) / (e**2)\nn  = n0 / (1 + (n0 - 1)/N)\nn = math.ceil(n)\n\nfraction = n / N\nseed = 42\n\n\nsample_df = df_spark.sample(withReplacement=False, fraction=fraction, seed=seed)\n\nsample_df = sample_df.limit(n).toPandas()\nprint(\"Rows :\", n)\nprint(\"Columns:\", len(sample_df.columns))\nprint(\"Sampling fraction :\", fraction)\nsample_df.info()\nsample_df.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T10:42:19.099732Z","iopub.execute_input":"2025-12-30T10:42:19.100071Z","iopub.status.idle":"2025-12-30T10:43:33.394325Z","shell.execute_reply.started":"2025-12-30T10:42:19.100045Z","shell.execute_reply":"2025-12-30T10:43:33.392918Z"}},"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Rows : 16554\nColumns: 46\nSampling fraction : 0.0021419715402708505\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 16437 entries, 0 to 16436\nData columns (total 46 columns):\n #   Column                 Non-Null Count  Dtype         \n---  ------                 --------------  -----         \n 0   ID                     16437 non-null  object        \n 1   Source                 16437 non-null  object        \n 2   Severity               16437 non-null  int32         \n 3   Start_Time             16437 non-null  datetime64[ns]\n 4   End_Time               16437 non-null  datetime64[ns]\n 5   Start_Lat              16437 non-null  float64       \n 6   Start_Lng              16437 non-null  float64       \n 7   End_Lat                9147 non-null   float64       \n 8   End_Lng                9147 non-null   float64       \n 9   Distance(mi)           16437 non-null  float64       \n 10  Description            16437 non-null  object        \n 11  Street                 16416 non-null  object        \n 12  City                   16437 non-null  object        \n 13  County                 16437 non-null  object        \n 14  State                  16437 non-null  object        \n 15  Zipcode                16434 non-null  object        \n 16  Country                16437 non-null  object        \n 17  Timezone               16419 non-null  object        \n 18  Airport_Code           16387 non-null  object        \n 19  Weather_Timestamp      16188 non-null  datetime64[ns]\n 20  Temperature(F)         16091 non-null  float64       \n 21  Wind_Chill(F)          12190 non-null  float64       \n 22  Humidity(%)            16069 non-null  float64       \n 23  Pressure(in)           16145 non-null  float64       \n 24  Visibility(mi)         16070 non-null  float64       \n 25  Wind_Direction         16076 non-null  object        \n 26  Wind_Speed(mph)        15217 non-null  float64       \n 27  Precipitation(in)      11740 non-null  float64       \n 28  Weather_Condition      16088 non-null  object        \n 29  Amenity                16437 non-null  bool          \n 30  Bump                   16437 non-null  bool          \n 31  Crossing               16437 non-null  bool          \n 32  Give_Way               16437 non-null  bool          \n 33  Junction               16437 non-null  bool          \n 34  No_Exit                16437 non-null  bool          \n 35  Railway                16437 non-null  bool          \n 36  Roundabout             16437 non-null  bool          \n 37  Station                16437 non-null  bool          \n 38  Stop                   16437 non-null  bool          \n 39  Traffic_Calming        16437 non-null  bool          \n 40  Traffic_Signal         16437 non-null  bool          \n 41  Turning_Loop           16437 non-null  bool          \n 42  Sunrise_Sunset         16386 non-null  object        \n 43  Civil_Twilight         16386 non-null  object        \n 44  Nautical_Twilight      16386 non-null  object        \n 45  Astronomical_Twilight  16386 non-null  object        \ndtypes: bool(13), datetime64[ns](3), float64(12), int32(1), object(17)\nmemory usage: 4.3+ MB\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"       ID   Source  Severity          Start_Time            End_Time  \\\n0  A-1644  Source2         2 2016-06-29 10:41:13 2016-06-29 11:11:13   \n1  A-1825  Source2         3 2016-06-30 19:15:13 2016-06-30 20:30:13   \n2  A-2100  Source2         2 2016-07-05 00:10:15 2016-07-05 00:40:15   \n3  A-3711  Source2         2 2016-07-21 18:48:47 2016-07-21 20:03:47   \n4  A-4410  Source2         2 2016-07-27 11:19:17 2016-07-27 11:49:17   \n\n   Start_Lat   Start_Lng  End_Lat  End_Lng  Distance(mi)  ... Roundabout  \\\n0  37.335892 -121.848213      NaN      NaN           0.0  ...      False   \n1  37.784817 -121.306778      NaN      NaN           0.0  ...      False   \n2  38.272430 -122.670288      NaN      NaN           0.0  ...      False   \n3  38.236065 -121.051071      NaN      NaN           0.0  ...      False   \n4  38.733231 -120.571091      NaN      NaN           0.0  ...      False   \n\n  Station   Stop Traffic_Calming Traffic_Signal Turning_Loop Sunrise_Sunset  \\\n0   False  False           False          False        False            Day   \n1   False  False           False          False        False            Day   \n2   False  False           False          False        False          Night   \n3   False  False           False          False        False            Day   \n4   False  False           False          False        False            Day   \n\n  Civil_Twilight Nautical_Twilight Astronomical_Twilight  \n0            Day               Day                   Day  \n1            Day               Day                   Day  \n2          Night             Night                 Night  \n3            Day               Day                   Day  \n4            Day               Day                   Day  \n\n[5 rows x 46 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Source</th>\n      <th>Severity</th>\n      <th>Start_Time</th>\n      <th>End_Time</th>\n      <th>Start_Lat</th>\n      <th>Start_Lng</th>\n      <th>End_Lat</th>\n      <th>End_Lng</th>\n      <th>Distance(mi)</th>\n      <th>...</th>\n      <th>Roundabout</th>\n      <th>Station</th>\n      <th>Stop</th>\n      <th>Traffic_Calming</th>\n      <th>Traffic_Signal</th>\n      <th>Turning_Loop</th>\n      <th>Sunrise_Sunset</th>\n      <th>Civil_Twilight</th>\n      <th>Nautical_Twilight</th>\n      <th>Astronomical_Twilight</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A-1644</td>\n      <td>Source2</td>\n      <td>2</td>\n      <td>2016-06-29 10:41:13</td>\n      <td>2016-06-29 11:11:13</td>\n      <td>37.335892</td>\n      <td>-121.848213</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>Day</td>\n      <td>Day</td>\n      <td>Day</td>\n      <td>Day</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A-1825</td>\n      <td>Source2</td>\n      <td>3</td>\n      <td>2016-06-30 19:15:13</td>\n      <td>2016-06-30 20:30:13</td>\n      <td>37.784817</td>\n      <td>-121.306778</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>Day</td>\n      <td>Day</td>\n      <td>Day</td>\n      <td>Day</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A-2100</td>\n      <td>Source2</td>\n      <td>2</td>\n      <td>2016-07-05 00:10:15</td>\n      <td>2016-07-05 00:40:15</td>\n      <td>38.272430</td>\n      <td>-122.670288</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>Night</td>\n      <td>Night</td>\n      <td>Night</td>\n      <td>Night</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A-3711</td>\n      <td>Source2</td>\n      <td>2</td>\n      <td>2016-07-21 18:48:47</td>\n      <td>2016-07-21 20:03:47</td>\n      <td>38.236065</td>\n      <td>-121.051071</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>Day</td>\n      <td>Day</td>\n      <td>Day</td>\n      <td>Day</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A-4410</td>\n      <td>Source2</td>\n      <td>2</td>\n      <td>2016-07-27 11:19:17</td>\n      <td>2016-07-27 11:49:17</td>\n      <td>38.733231</td>\n      <td>-120.571091</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>Day</td>\n      <td>Day</td>\n      <td>Day</td>\n      <td>Day</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 46 columns</p>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"## Data Cleaning & Quality Assurance","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n\nID_COL = \"ID\"\n\nTIME_COLS = [\"Start_Time\", \"End_Time\", \"Weather_Timestamp\"]\nGEO_CRITICAL = [\"Start_Lat\", \"Start_Lng\"]\nGEO_OPTIONAL = [\"End_Lat\", \"End_Lng\"]\n\nWEATHER_NUM = [\n    \"Temperature(F)\", \"Wind_Chill(F)\", \"Humidity(%)\", \"Pressure(in)\",\n    \"Visibility(mi)\", \"Wind_Speed(mph)\", \"Precipitation(in)\"\n]\nWEATHER_CAT = [\"Weather_Condition\", \"Wind_Direction\"]\n\nTWILIGHT_COLS = [\"Sunrise_Sunset\", \"Civil_Twilight\", \"Nautical_Twilight\", \"Astronomical_Twilight\"]\n\nBOOL_COLS = [\n    \"Amenity\",\"Bump\",\"Crossing\",\"Give_Way\",\"Junction\",\"No_Exit\",\"Railway\",\n    \"Roundabout\",\"Station\",\"Stop\",\"Traffic_Calming\",\"Traffic_Signal\",\"Turning_Loop\"\n]\n\nLOCATION_COLS = [\"City\", \"County\", \"State\", \"Zipcode\", \"Country\", \"Timezone\", \"Airport_Code\", \"Street\"]\n\nRANGE_RULES = {\n    \"Visibility(mi)\": (0, 60),       \n    \"Temperature(F)\": (-60, 140),    \n    \"Distance(mi)\": (0, None),       \n}\n\n\ndef structural_validation(df: pd.DataFrame) -> pd.DataFrame:\n    print(\"=== Structural Validation ===\")\n    print(f\"Shape: {df.shape[0]:,} rows × {df.shape[1]:,} columns\")\n\n    expected = set([ID_COL] + TIME_COLS + GEO_CRITICAL + GEO_OPTIONAL + WEATHER_NUM + WEATHER_CAT + BOOL_COLS)\n    missing_expected = sorted(list(expected - set(df.columns)))\n    if missing_expected:\n        print(\"WARNING: Missing expected columns:\")\n        for c in missing_expected:\n            print(\" -\", c)\n\n    for c in TIME_COLS:\n        if c in df.columns:\n            df[c] = pd.to_datetime(df[c], errors=\"coerce\", utc=False)\n\n    for c in BOOL_COLS:\n        if c in df.columns:\n            if df[c].dtype != bool:\n                df[c] = (\n                    df[c]\n                    .replace({1: True, 0: False, \"1\": True, \"0\": False, \"True\": True, \"False\": False})\n                    .astype(\"boolean\")\n                    .fillna(False)\n                    .astype(bool)\n                )\n\n    print(\"\\nDtype summary (top):\")\n    print(df.dtypes.value_counts())\n\n    return df\n\n\ndef missing_value_report(df: pd.DataFrame) -> pd.DataFrame:\n    print(\"\\n=== Missing Value Analysis ===\")\n\n    miss = df.isna().sum()\n    miss_pct = (miss / len(df) * 100).round(2)\n    report = (\n        pd.DataFrame({\"missing_count\": miss, \"missing_pct\": miss_pct})\n        .sort_values([\"missing_count\", \"missing_pct\"], ascending=False)\n    )\n\n    def classify(col: str) -> str:\n        if col in GEO_CRITICAL or col in TIME_COLS:\n            return \"Critical\"\n        if col in GEO_OPTIONAL:\n            return \"Optional\"\n        if col in WEATHER_NUM or col in WEATHER_CAT:\n            return \"Contextual (Weather)\"\n        return \"Other\"\n\n    report[\"missing_class\"] = [classify(c) for c in report.index]\n\n    print(report.head(15))\n\n    return report\n\n\ndef plot_missingness(df: pd.DataFrame, top_n: int = 25, matrix_sample: int = 3000) -> None:\n    \"\"\"\n    Fast missingness visuals:\n    - Bar chart for top missing columns\n    - Missingness matrix heatmap on a sample for speed\n    \"\"\"\n    miss = df.isna().mean().sort_values(ascending=False)\n    miss_top = miss.head(top_n)\n\n    plt.figure()\n    plt.bar(miss_top.index, miss_top.values)\n    plt.xticks(rotation=75, ha=\"right\")\n    plt.ylabel(\"Missing fraction\")\n    plt.title(f\"Top {top_n} columns by missingness\")\n    plt.tight_layout()\n    plt.show()\n\n    n = min(len(df), matrix_sample)\n    sample = df.sample(n=n, random_state=42)\n    mask = sample.isna().to_numpy(dtype=np.uint8)  # 1 = missing\n\n    plt.figure()\n    plt.imshow(mask, aspect=\"auto\", interpolation=\"nearest\")\n    plt.title(f\"Missingness matrix (sample n={n}) — 1=missing\")\n    plt.xlabel(\"Columns\")\n    plt.ylabel(\"Rows (sample)\")\n    plt.tight_layout()\n    plt.show()\n\ndef month_to_season(month: int) -> str:\n    if month in (12, 1, 2):\n        return \"Winter\"\n    if month in (3, 4, 5):\n        return \"Spring\"\n    if month in (6, 7, 8):\n        return \"Summer\"\n    return \"Fall\"\n\n\ndef treat_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n    print(\"\\n=== Missing Value Treatment Strategy ===\")\n\n    before = len(df)\n\n    if all(c in df.columns for c in GEO_CRITICAL):\n        df = df.dropna(subset=GEO_CRITICAL)\n        print(f\"Dropped {before - len(df):,} rows missing {GEO_CRITICAL}\")\n    else:\n        print(\"WARNING: Critical geo columns not fully present; skip geo drop.\")\n\n    for c in WEATHER_CAT:\n        if c in df.columns:\n            df[c] = df[c].astype(\"string\")\n            df[c] = df[c].fillna(\"Unknown\")\n\n    for c in WEATHER_NUM:\n        if c in df.columns:\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n\n    if \"Start_Time\" in df.columns:\n        month = df[\"Start_Time\"].dt.month\n        df[\"_Season\"] = month.map(month_to_season).astype(\"string\")\n    else:\n        df[\"_Season\"] = \"Unknown\"\n\n    group_keys = []\n    if \"State\" in df.columns:\n        group_keys.append(\"State\")\n    group_keys.append(\"_Season\")\n\n    for c in WEATHER_NUM:\n        if c in df.columns:\n            med = df.groupby(group_keys, dropna=False)[c].transform(\"median\")\n            global_med = df[c].median()\n            df[c] = df[c].fillna(med).fillna(global_med)\n\n    for c in TWILIGHT_COLS:\n        if c in df.columns:\n            df[c] = df[c].astype(\"string\")\n\n    df = df.drop(columns=[\"_Season\"], errors=\"ignore\")\n\n    return df\n\n\ndef integrity_checks(df: pd.DataFrame) -> pd.DataFrame:\n    print(\"\\n=== Duplicate & Integrity Checks ===\")\n\n    if ID_COL in df.columns:\n        dup_mask = df[ID_COL].duplicated(keep=\"first\")\n        dup_count = int(dup_mask.sum())\n        if dup_count > 0:\n            print(f\"Found {dup_count:,} duplicate IDs. Keeping first occurrence.\")\n            df = df.loc[~dup_mask].copy()\n        else:\n            print(\"No duplicate IDs found.\")\n    else:\n        print(\"WARNING: ID column not found; skip duplicate ID check.\")\n\n    if all(c in df.columns for c in [\"Start_Time\", \"End_Time\"]):\n        bad_time = df[\"End_Time\"].notna() & df[\"Start_Time\"].notna() & (df[\"End_Time\"] < df[\"Start_Time\"])\n        bad_time_count = int(bad_time.sum())\n        if bad_time_count > 0:\n            print(f\"Found {bad_time_count:,} rows with End_Time < Start_Time. Dropping them.\")\n            df = df.loc[~bad_time].copy()\n        else:\n            print(\"Time consistency OK (no End_Time < Start_Time).\")\n    else:\n        print(\"WARNING: Start_Time/End_Time missing; skip time consistency check.\")\n\n    def flag_out_of_range(series: pd.Series, low, high):\n        mask = pd.Series(False, index=series.index)\n        if low is not None:\n            mask |= series < low\n        if high is not None:\n            mask |= series > high\n        return mask\n\n    for col, (low, high) in RANGE_RULES.items():\n        if col in df.columns:\n            s = pd.to_numeric(df[col], errors=\"coerce\")\n            bad = flag_out_of_range(s, low, high)\n            bad_count = int(bad.sum())\n            if bad_count > 0:\n                print(f\"{col}: {bad_count:,} out-of-range values detected. Setting them to NaN.\")\n                df.loc[bad, col] = np.nan\n\n    return df\n\n\n\ndef clean_us_accidents(df: pd.DataFrame, plot_missing: bool = True) -> tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Returns:\n      df_clean: cleaned dataframe\n      missing_report: missingness report BEFORE treatment (for documentation)\n    \"\"\"\n    df = df.copy()\n\n    df = structural_validation(df)\n\n    missing_report = missing_value_report(df)\n    if plot_missing:\n        plot_missingness(df, top_n=25, matrix_sample=3000)\n\n    df = treat_missing_values(df)\n    df = integrity_checks(df)\n\n    print(\"\\n=== CLEANING COMPLETE ===\")\n    print(f\"Final shape: {df.shape[0]:,} rows × {df.shape[1]:,} columns\")\n\n\n    crit_nulls = {c: int(df[c].isna().sum()) for c in GEO_CRITICAL if c in df.columns}\n    if crit_nulls:\n        print(\"Critical null counts (should be 0):\", crit_nulls)\n\n    return df, missing_report\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T11:26:28.445959Z","iopub.execute_input":"2025-12-30T11:26:28.446263Z","iopub.status.idle":"2025-12-30T11:26:28.813455Z","shell.execute_reply.started":"2025-12-30T11:26:28.446236Z","shell.execute_reply":"2025-12-30T11:26:28.812631Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"code","source":"\n\nimport numpy as np\nimport pandas as pd\nimport re\n\ndef month_to_season(month: pd.Series) -> pd.Series:\n    return pd.Series(\n        np.select(\n            [\n                month.isin([12, 1, 2]),\n                month.isin([3, 4, 5]),\n                month.isin([6, 7, 8]),\n                month.isin([9, 10, 11]),\n            ],\n            [\"Winter\", \"Spring\", \"Summer\", \"Fall\"],\n            default=\"Unknown\",\n        ),\n        index=month.index,\n        dtype=\"string\",\n    )\n\ndef _contains_any(text: pd.Series, patterns: list[str]) -> pd.Series:\n    \"\"\"Vectorized regex search. Returns boolean Series.\"\"\"\n    rgx = \"(\" + \"|\".join(patterns) + \")\"\n    return text.str.contains(rgx, case=False, na=False, regex=True)\n\nWEATHER_PATTERNS = {\n    \"Snow / Ice\": [\n        r\"snow\", r\"blizzard\", r\"sleet\", r\"ice\", r\"freezing rain\", r\"hail\", r\"wintry\",\n        r\"mixed\", r\"graupel\"\n    ],\n    \"Rain / Thunderstorm\": [\n        r\"rain\", r\"drizzle\", r\"shower\", r\"thunder\", r\"t-storm\", r\"storm\", r\"tornado\",\n        r\"tropical\", r\"heavy rain\"\n    ],\n    \"Fog / Haze / Smoke\": [\n        r\"fog\", r\"mist\", r\"haze\", r\"smoke\", r\"volcanic ash\", r\"dust\", r\"sand\"\n    ],\n    \"Cloudy\": [\n        r\"cloud\", r\"overcast\"\n    ],\n    \"Clear\": [\n        r\"clear\", r\"fair\"\n    ],\n}\n\ndef bucket_weather(weather: pd.Series) -> pd.Series:\n    \"\"\"\n    Maps Weather_Condition free text into 5 groups:\n    Clear / Cloudy / Rain-Thunder / Snow-Ice / Fog-Haze-Smoke / Extreme-Unknown\n    \"\"\"\n    w = weather.astype(\"string\").fillna(\"Unknown\").str.strip()\n\n    extreme = _contains_any(w, [\n        r\"hurricane\", r\"tornado\", r\"thunderstorm\", r\"severe\", r\"squall\", r\"microburst\",\n        r\"ice storm\", r\"dust storm\", r\"sandstorm\", r\"smoke\"\n    ])\n\n    out = pd.Series(\"Extreme / Unknown\", index=w.index, dtype=\"string\")\n\n\n    snow = _contains_any(w, WEATHER_PATTERNS[\"Snow / Ice\"])\n    rain = _contains_any(w, WEATHER_PATTERNS[\"Rain / Thunderstorm\"])\n    fog  = _contains_any(w, WEATHER_PATTERNS[\"Fog / Haze / Smoke\"])\n    cloudy = _contains_any(w, WEATHER_PATTERNS[\"Cloudy\"])\n    clear  = _contains_any(w, WEATHER_PATTERNS[\"Clear\"])\n\n    out.loc[clear] = \"Clear / Cloudy\"   \n    out.loc[cloudy] = \"Clear / Cloudy\"\n\n\n    out.loc[fog] = \"Fog / Haze / Smoke\"\n    out.loc[rain] = \"Rain / Thunderstorm\"\n    out.loc[snow] = \"Snow / Ice\"\n\n    out.loc[extreme] = \"Extreme / Unknown\"\n    out.loc[w.eq(\"Unknown\")] = \"Extreme / Unknown\"\n\n    return out\n\nTIME_COL = \"Start_Time\"\nEND_TIME_COL = \"End_Time\"\n\nWEATHER_COL = \"Weather_Condition\"\nVIS_COL = \"Visibility(mi)\"\nWIND_COL = \"Wind_Speed(mph)\"\nTEMP_COL = \"Temperature(F)\"\n\nBOOLS_INTERSECTION = [\"Junction\", \"Traffic_Signal\", \"Crossing\"]\nBOOLS_TRAFFIC_CONTROL = [\"Stop\", \"Give_Way\", \"No_Exit\", \"Traffic_Calming\", \"Traffic_Signal\"]\nBOOLS_URBAN_PROX = [\"Amenity\", \"Station\", \"Railway\", \"Roundabout\"]  \n\ndef feature_engineering(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Adds engineered features for time, weather, and infrastructure.\n    Returns a new dataframe with extra columns.\n    \"\"\"\n    df = df.copy()\n\n    if TIME_COL in df.columns:\n        st = pd.to_datetime(df[TIME_COL], errors=\"coerce\")\n        df[\"start_hour\"] = st.dt.hour.astype(\"Int16\")\n        df[\"day_of_week\"] = st.dt.dayofweek.astype(\"Int8\")  # Mon=0..Sun=6\n        df[\"day_name\"] = st.dt.day_name().astype(\"string\")\n        df[\"is_weekend\"] = st.dt.dayofweek.isin([5, 6]).astype(\"Int8\")  # 1/0 (fast, model-friendly)\n        df[\"month\"] = st.dt.month.astype(\"Int8\")\n        df[\"season\"] = month_to_season(st.dt.month).astype(\"category\")\n\n        hour = st.dt.hour\n        df[\"rush_morning\"] = hour.between(7, 9, inclusive=\"both\").astype(\"Int8\")\n        df[\"rush_evening\"] = hour.between(16, 19, inclusive=\"both\").astype(\"Int8\")\n\n    if TIME_COL in df.columns and END_TIME_COL in df.columns:\n        et = pd.to_datetime(df[END_TIME_COL], errors=\"coerce\")\n        duration_min = (et - st).dt.total_seconds() / 60.0\n\n        df[\"accident_duration_min\"] = duration_min.where(duration_min.between(0, 24*60), np.nan).astype(\"float32\")\n\n    if WEATHER_COL in df.columns:\n        df[\"weather_group\"] = bucket_weather(df[WEATHER_COL]).astype(\"category\")\n\n    if VIS_COL in df.columns:\n        vis = pd.to_numeric(df[VIS_COL], errors=\"coerce\")\n        df[\"risk_low_visibility\"] = (vis < 2).astype(\"Int8\")\n    else:\n        df[\"risk_low_visibility\"] = pd.Series(np.nan, index=df.index, dtype=\"float32\")\n\n \n    if WIND_COL in df.columns:\n        wind = pd.to_numeric(df[WIND_COL], errors=\"coerce\")\n        df[\"risk_high_wind\"] = (wind > 20).astype(\"Int8\")\n    else:\n        df[\"risk_high_wind\"] = pd.Series(np.nan, index=df.index, dtype=\"float32\")\n\n   \n    if TEMP_COL in df.columns:\n        temp = pd.to_numeric(df[TEMP_COL], errors=\"coerce\")\n        df[\"risk_freezing_temp\"] = (temp < 32).astype(\"Int8\")\n    else:\n        df[\"risk_freezing_temp\"] = pd.Series(np.nan, index=df.index, dtype=\"float32\")\n\n    df[\"weather_risk_score\"] = (\n        df[\"risk_low_visibility\"].fillna(0).astype(\"int16\")\n        + df[\"risk_high_wind\"].fillna(0).astype(\"int16\")\n        + df[\"risk_freezing_temp\"].fillna(0).astype(\"int16\")\n    ).astype(\"Int8\")\n\n    for c in (BOOLS_INTERSECTION + BOOLS_TRAFFIC_CONTROL + BOOLS_URBAN_PROX):\n        if c not in df.columns:\n            df[c] = False\n\n    df[\"intersection_complexity_count\"] = (\n        df[BOOLS_INTERSECTION].astype(\"int8\").sum(axis=1)\n    ).astype(\"Int8\")\n\n    df[\"intersection_complexity_level\"] = pd.Categorical(\n        np.select(\n            [\n                df[\"intersection_complexity_count\"].eq(0),\n                df[\"intersection_complexity_count\"].eq(1),\n                df[\"intersection_complexity_count\"].ge(2),\n            ],\n            [\"Low\", \"Medium\", \"High\"],\n            default=\"Low\",\n        ),\n        categories=[\"Low\", \"Medium\", \"High\"],\n        ordered=True,\n    )\n\n    df[\"traffic_control_presence\"] = (\n        df[BOOLS_TRAFFIC_CONTROL].any(axis=1)\n    ).astype(\"Int8\")\n\n    df[\"urban_infra_proximity\"] = (\n        df[BOOLS_URBAN_PROX].any(axis=1)\n    ).astype(\"Int8\")\n\n\n    road_bool_cols = list(dict.fromkeys(BOOLS_INTERSECTION + BOOLS_TRAFFIC_CONTROL + BOOLS_URBAN_PROX))\n    df[\"road_complexity_score\"] = df[road_bool_cols].astype(\"int8\").sum(axis=1).astype(\"Int16\")\n\n    if \"Traffic_Signal\" in df.columns:\n        df[\"traffic_signal_cat\"] = pd.Categorical(\n            np.where(df[\"Traffic_Signal\"], \"Yes\", \"No\"),\n            categories=[\"No\", \"Yes\"],\n            ordered=True\n        )\n\n    return df\n\n\ndef engineered_feature_summary(df: pd.DataFrame) -> None:\n    engineered_cols = [\n        \"start_hour\",\"day_of_week\",\"day_name\",\"is_weekend\",\"month\",\"season\",\n        \"rush_morning\",\"rush_evening\",\"accident_duration_min\",\n        \"weather_group\",\"risk_low_visibility\",\"risk_high_wind\",\"risk_freezing_temp\",\"weather_risk_score\",\n        \"intersection_complexity_count\",\"intersection_complexity_level\",\n        \"traffic_control_presence\",\"urban_infra_proximity\",\"road_complexity_score\",\"traffic_signal_cat\"\n    ]\n    present = [c for c in engineered_cols if c in df.columns]\n    print(\"Engineered columns present:\", len(present))\n    print(df[present].dtypes)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T11:33:00.756913Z","iopub.execute_input":"2025-12-30T11:33:00.757469Z","iopub.status.idle":"2025-12-30T11:33:00.785115Z","shell.execute_reply.started":"2025-12-30T11:33:00.757438Z","shell.execute_reply":"2025-12-30T11:33:00.784201Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Exploratory Data Analysis (EDA)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nTARGET = \"Severity\"\nDIST_COL = \"Distance(mi)\"\nDUR_COL  = \"accident_duration_min\"\n\nHOUR_COL = \"start_hour\"\nDOW_COL  = \"day_of_week\"     \nDOW_NAME = \"day_name\"\nSEASON_COL = \"season\"\n\nWEATHER_GRP = \"weather_group\"\nVIS_COL = \"Visibility(mi)\"\n\nROAD_FEATURES = [\"Junction\", \"Traffic_Signal\", \"Crossing\"]  \n\nDOW_ORDER = [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n\n\n\ndef _ensure_cols(df: pd.DataFrame, cols: list[str]) -> None:\n    missing = [c for c in cols if c not in df.columns]\n    if missing:\n        raise KeyError(f\"Missing required columns: {missing}\")\n\ndef _clean_numeric(s: pd.Series) -> pd.Series:\n    return pd.to_numeric(s, errors=\"coerce\")\n\ndef _boxplot_by_severity(df: pd.DataFrame, y: str, title: str, ylab: str, clip_q=(0.01, 0.99)) -> None:\n    \"\"\"Fast boxplot-like view using quantile clipping + grouped stats (no seaborn).\"\"\"\n    x = df[TARGET]\n    v = _clean_numeric(df[y])\n\n\n    lo, hi = v.quantile(clip_q[0]), v.quantile(clip_q[1])\n    v = v.clip(lo, hi)\n\n    severities = sorted(df[TARGET].dropna().unique())\n    data = [v[df[TARGET] == s].dropna().values for s in severities]\n\n    plt.figure()\n    plt.boxplot(data, labels=[str(s) for s in severities], showfliers=False)\n    plt.title(title)\n    plt.xlabel(\"Severity\")\n    plt.ylabel(ylab)\n    plt.tight_layout()\n    plt.show()\n\ndef _bar_counts(series: pd.Series, title: str, xlabel: str = \"\", ylabel: str = \"Count\") -> None:\n    counts = series.value_counts(dropna=False)\n    plt.figure()\n    plt.bar(counts.index.astype(str), counts.values)\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.tight_layout()\n    plt.show()\n\ndef _bar_from_table(tab: pd.Series, title: str, xlabel: str = \"\", ylabel: str = \"Count\", rotate=0) -> None:\n    plt.figure()\n    plt.bar(tab.index.astype(str), tab.values)\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.xticks(rotation=rotate, ha=\"right\" if rotate else \"center\")\n    plt.tight_layout()\n    plt.show()\n\ndef _heatmap(pivot: pd.DataFrame, title: str, xlabel: str, ylabel: str) -> None:\n    arr = pivot.values\n    plt.figure()\n    plt.imshow(arr, aspect=\"auto\", interpolation=\"nearest\")\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.xticks(range(pivot.shape[1]), pivot.columns.astype(str), rotation=0)\n    plt.yticks(range(pivot.shape[0]), pivot.index.astype(str))\n    plt.colorbar(label=\"Count\")\n    plt.tight_layout()\n    plt.show()\n\ndef _rate_table(df: pd.DataFrame, feature: str) -> pd.DataFrame:\n    \"\"\"\n    Returns: counts + severity distribution for feature True/False.\n    \"\"\"\n    tmp = df[[feature, TARGET]].copy()\n    tmp[feature] = tmp[feature].astype(bool)\n\n    counts = tmp[feature].value_counts().rename(\"n\")\n    sev_dist = (\n        tmp.groupby(feature)[TARGET]\n        .value_counts(normalize=True)\n        .rename(\"share\")\n        .reset_index()\n        .pivot(index=feature, columns=TARGET, values=\"share\")\n        .fillna(0)\n    )\n    out = pd.concat([counts, sev_dist], axis=1).fillna(0)\n    out.index = out.index.map({False: \"False\", True: \"True\"})\n    return out\n\n\ndef run_eda(df: pd.DataFrame) -> dict:\n    _ensure_cols(df, [TARGET, HOUR_COL, DOW_COL, DOW_NAME, SEASON_COL, WEATHER_GRP])\n\n    outputs = {}\n\n    print(\"\\n=== Severity Distribution Analysis ===\")\n\n    sev_counts = df[TARGET].value_counts().sort_index()\n    outputs[\"severity_counts\"] = sev_counts\n    _bar_from_table(sev_counts, title=\"Severity distribution (counts)\", xlabel=\"Severity\", ylabel=\"Accidents\")\n\n\n    if DIST_COL in df.columns:\n        _boxplot_by_severity(\n            df, y=DIST_COL,\n            title=\"Distance(mi) vs Severity (clipped for readability)\",\n            ylab=\"Distance (mi)\"\n        )\n        outputs[\"distance_summary_by_severity\"] = (\n            df.groupby(TARGET)[DIST_COL]\n              .agg(n=\"count\", mean=\"mean\", median=\"median\", p90=lambda x: x.quantile(0.90))\n        )\n\n    if DUR_COL in df.columns:\n        _boxplot_by_severity(\n            df, y=DUR_COL,\n            title=\"Accident duration (min) vs Severity (clipped for readability)\",\n            ylab=\"Duration (minutes)\"\n        )\n        outputs[\"duration_summary_by_severity\"] = (\n            df.groupby(TARGET)[DUR_COL]\n              .agg(n=\"count\", mean=\"mean\", median=\"median\", p90=lambda x: x.quantile(0.90))\n        )\n\n    road_tables = {}\n    for feat in ROAD_FEATURES:\n        if feat in df.columns:\n            road_tables[feat] = _rate_table(df, feat)\n            print(f\"\\nSeverity share by {feat} (True/False):\")\n            print(road_tables[feat])\n    outputs[\"severity_by_road_feature\"] = road_tables\n\n\n    print(\"\\n===  Temporal Pattern Analysis ===\")\n\n    hour_counts = df[HOUR_COL].value_counts().sort_index()\n    outputs[\"accidents_by_hour\"] = hour_counts\n    _bar_from_table(hour_counts, title=\"Accidents by hour of day\", xlabel=\"Hour\", ylabel=\"Accidents\")\n\n    dow_counts = (\n        df[DOW_NAME].value_counts()\n        .reindex(DOW_ORDER)\n        .fillna(0)\n        .astype(int)\n    )\n    outputs[\"accidents_by_day\"] = dow_counts\n    _bar_from_table(dow_counts, title=\"Accidents by day of week\", xlabel=\"Day\", ylabel=\"Accidents\", rotate=30)\n\n    pivot_hw = (\n        df.pivot_table(index=DOW_NAME, columns=HOUR_COL, values=TARGET, aggfunc=\"size\", fill_value=0)\n        .reindex(DOW_ORDER)\n    )\n    outputs[\"heatmap_hour_weekday\"] = pivot_hw\n    _heatmap(pivot_hw, title=\"Accident frequency heatmap: Day × Hour\", xlabel=\"Hour\", ylabel=\"Day\")\n\n    season_counts = df[SEASON_COL].astype(\"string\").value_counts()\n    \n    season_order = [\"Winter\", \"Spring\", \"Summer\", \"Fall\", \"Unknown\"]\n    season_counts = season_counts.reindex([s for s in season_order if s in season_counts.index]).fillna(0).astype(int)\n    outputs[\"accidents_by_season\"] = season_counts\n    _bar_from_table(season_counts, title=\"Accidents by season\", xlabel=\"Season\", ylabel=\"Accidents\")\n\n\n    print(\"\\n===  Weather Condition Analysis ===\")\n    weather_counts = df[WEATHER_GRP].astype(\"string\").value_counts()\n    outputs[\"accidents_by_weather_group\"] = weather_counts\n    _bar_from_table(weather_counts, title=\"Accidents by weather group\", xlabel=\"Weather group\", ylabel=\"Accidents\", rotate=25)\n\n    weather_sev = (\n        df.groupby(WEATHER_GRP)[TARGET]\n          .agg(n=\"count\", mean_severity=\"mean\", median_severity=\"median\")\n          .sort_values(\"n\", ascending=False)\n    )\n    outputs[\"severity_by_weather_group\"] = weather_sev\n    print(\"\\nSeverity by weather group (n, mean, median):\")\n    print(weather_sev)\n\n    if VIS_COL in df.columns:\n        vis = _clean_numeric(df[VIS_COL])\n        tmp = df[[TARGET]].copy()\n        tmp[\"Visibility(mi)\"] = vis\n\n        bins = [-np.inf, 1, 2, 5, 10, 20, np.inf]\n        labels = [\"<=1\", \"1-2\", \"2-5\", \"5-10\", \"10-20\", \"20+\"]\n        tmp[\"vis_bin\"] = pd.cut(tmp[\"Visibility(mi)\"], bins=bins, labels=labels)\n\n        vis_sev = (\n            tmp.groupby(\"vis_bin\")[TARGET]\n               .agg(n=\"count\", mean_severity=\"mean\", median_severity=\"median\")\n        )\n        outputs[\"severity_by_visibility_bin\"] = vis_sev\n        print(\"\\nSeverity by visibility bin:\")\n        print(vis_sev)\n\n        plt.figure()\n        plt.bar(vis_sev.index.astype(str), vis_sev[\"mean_severity\"].values)\n        plt.title(\"Mean Severity by Visibility Bin\")\n        plt.xlabel(\"Visibility (mi) bin\")\n        plt.ylabel(\"Mean severity\")\n        plt.tight_layout()\n        plt.show()\n\n    print(\"\\n=== Road Condition Analysis ===\")\n\n    road_freq = {}\n    for feat in ROAD_FEATURES:\n        if feat in df.columns:\n            n_true = int(df[feat].astype(bool).sum())\n            n_total = len(df)\n            road_freq[feat] = {\"n_true\": n_true, \"share_true\": n_true / n_total if n_total else np.nan}\n\n            counts = df[feat].astype(bool).value_counts().reindex([False, True]).fillna(0).astype(int)\n            _bar_from_table(counts, title=f\"Accidents near {feat}: True vs False\", xlabel=feat, ylabel=\"Accidents\")\n\n            sev_by = df.groupby(df[feat].astype(bool))[TARGET].mean()\n            sev_by = sev_by.reindex([False, True])\n            plt.figure()\n            plt.bar(sev_by.index.astype(str), sev_by.values)\n            plt.title(f\"Mean Severity by {feat} (True/False)\")\n            plt.xlabel(feat)\n            plt.ylabel(\"Mean severity\")\n            plt.tight_layout()\n            plt.show()\n\n    outputs[\"road_feature_frequency\"] = pd.DataFrame(road_freq).T.sort_values(\"share_true\", ascending=False)\n\n    print(\"\\nRoad feature frequency (share True):\")\n    print(outputs[\"road_feature_frequency\"])\n\n    hypotheses = []\n    if len(hour_counts) > 0:\n        peak_hour = int(hour_counts.idxmax())\n        hypotheses.append(f\"Temporal rhythm: accidents peak around hour {peak_hour}:00, consistent with human commuting patterns.\")\n\n    if \"severity_by_visibility_bin\" in outputs and not outputs[\"severity_by_visibility_bin\"].empty:\n        \n        vs = outputs[\"severity_by_visibility_bin\"][\"mean_severity\"]\n        if vs.notna().any():\n            worst_bin = vs.idxmax()\n            best_bin = vs.idxmin()\n            hypotheses.append(f\"Visibility effect: mean severity is highest at visibility bin '{worst_bin}', supporting the idea that low visibility amplifies severity.\")\n \n    if isinstance(outputs.get(\"road_feature_frequency\"), pd.DataFrame) and not outputs[\"road_feature_frequency\"].empty:\n        top_feat = outputs[\"road_feature_frequency\"].index[0]\n        hypotheses.append(f\"Road context: '{top_feat}' occurs frequently near accidents (high share True), suggesting intersection/traffic-control environments concentrate incidents.\")\n\n    outputs[\"hypotheses\"] = hypotheses\n    print(\"\\n=== Pattern Discovery → Hypotheses ===\")\n    for i, h in enumerate(hypotheses, 1):\n        print(f\"{i}. {h}\")\n\n    return outputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T11:47:30.058637Z","iopub.execute_input":"2025-12-30T11:47:30.059626Z","iopub.status.idle":"2025-12-30T11:47:30.097036Z","shell.execute_reply.started":"2025-12-30T11:47:30.059578Z","shell.execute_reply":"2025-12-30T11:47:30.096049Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## GEOSPATIAL HOTSPOT ANALYSIS","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nLAT = \"Start_Lat\"\nLNG = \"Start_Lng\"\nSEV = \"Severity\"\nHOUR = \"start_hour\"\nWEATHER_GRP = \"weather_group\"\n\nSTATE = \"State\"\nCITY = \"City\"\n\nROAD_FEATURES = [\"Junction\", \"Traffic_Signal\", \"Crossing\"]\n\n\ndef _ensure_cols(df: pd.DataFrame, cols: list[str]) -> None:\n    missing = [c for c in cols if c not in df.columns]\n    if missing:\n        raise KeyError(f\"Missing required columns: {missing}\")\n\ndef _as_numeric(s: pd.Series) -> pd.Series:\n    return pd.to_numeric(s, errors=\"coerce\")\n\ndef _severity_weight(sev: pd.Series) -> pd.Series:\n    \"\"\"\n    Weight points by severity. Simple + explainable:\n    severity 1..4 -> weights 1..4\n    \"\"\"\n    return _as_numeric(sev).fillna(0).clip(0, 10)\n\ndef _make_grid(df: pd.DataFrame, lat_col=LAT, lng_col=LNG, grid_deg=0.05) -> pd.DataFrame:\n    \"\"\"\n    Grid the map by rounding lat/lng to a fixed resolution (degrees).\n    ~0.05 deg ~ 5-6 km. Adjust grid_deg for tighter/looser hotspots.\n    \"\"\"\n    out = df.copy()\n    out[\"_lat_bin\"] = (np.floor(out[lat_col] / grid_deg) * grid_deg).astype(\"float32\")\n    out[\"_lng_bin\"] = (np.floor(out[lng_col] / grid_deg) * grid_deg).astype(\"float32\")\n    out[\"_cell_id\"] = out[\"_lat_bin\"].astype(str) + \"_\" + out[\"_lng_bin\"].astype(str)\n    return out\n\ndef _top_k(series: pd.Series, k: int) -> list:\n    return series.value_counts().head(k).index.tolist()\n\n\ndef plot_national_scatter(df: pd.DataFrame, sample_n: int = 12000) -> None:\n    \"\"\"\n    Lightweight scatter for context (use sampling to stay fast).\n    \"\"\"\n    _ensure_cols(df, [LAT, LNG])\n\n    tmp = df[[LAT, LNG]].dropna()\n    if len(tmp) > sample_n:\n        tmp = tmp.sample(sample_n, random_state=42)\n\n    plt.figure()\n    plt.scatter(tmp[LNG].values, tmp[LAT].values, s=2)\n    plt.title(f\"National accident scatter (sample n={len(tmp):,})\")\n    plt.xlabel(\"Longitude\")\n    plt.ylabel(\"Latitude\")\n    plt.tight_layout()\n    plt.show()\n\n\ndef plot_national_hexbin(df: pd.DataFrame, gridsize: int = 70, weight_by_severity: bool = True) -> None:\n    \"\"\"\n    Hexbin density map. If weight_by_severity=True, bins reflect severity-weighted intensity.\n    \"\"\"\n    _ensure_cols(df, [LAT, LNG, SEV])\n\n    tmp = df[[LAT, LNG, SEV]].dropna()\n    x = tmp[LNG].values\n    y = tmp[LAT].values\n    C = _severity_weight(tmp[SEV]).values if weight_by_severity else None\n\n    plt.figure()\n    hb = plt.hexbin(\n        x, y,\n        C=C,\n        reduce_C_function=np.sum if weight_by_severity else None,\n        gridsize=gridsize,\n        mincnt=1\n    )\n    plt.colorbar(hb, label=(\"Severity-weighted count\" if weight_by_severity else \"Accident count\"))\n    plt.title(\"National accident density (hexbin)\" + (\" — weighted by severity\" if weight_by_severity else \"\"))\n    plt.xlabel(\"Longitude\")\n    plt.ylabel(\"Latitude\")\n    plt.tight_layout()\n    plt.show()\n\n\ndef plot_national_kde(df: pd.DataFrame, sample_n: int = 15000, weight_by_severity: bool = True, gridsize: int = 200) -> None:\n    \"\"\"\n    KDE map (optional). Uses SciPy if available; otherwise prints a message.\n    \"\"\"\n    try:\n        from scipy.stats import gaussian_kde\n    except Exception:\n        print(\"SciPy not available -> KDE skipped. Use hexbin instead.\")\n        return\n\n    _ensure_cols(df, [LAT, LNG, SEV])\n\n    tmp = df[[LAT, LNG, SEV]].dropna()\n    if len(tmp) > sample_n:\n        tmp = tmp.sample(sample_n, random_state=42)\n\n    x = tmp[LNG].values\n    y = tmp[LAT].values\n    w = _severity_weight(tmp[SEV]).values if weight_by_severity else None\n\n    xy = np.vstack([x, y])\n    kde = gaussian_kde(xy, weights=w)\n\n    xmin, xmax = x.min(), x.max()\n    ymin, ymax = y.min(), y.max()\n    xi = np.linspace(xmin, xmax, gridsize)\n    yi = np.linspace(ymin, ymax, gridsize)\n    xx, yy = np.meshgrid(xi, yi)\n    zz = kde(np.vstack([xx.ravel(), yy.ravel()])).reshape(xx.shape)\n\n    plt.figure()\n    plt.imshow(\n        zz,\n        origin=\"lower\",\n        aspect=\"auto\",\n        extent=[xmin, xmax, ymin, ymax]\n    )\n    plt.colorbar(label=(\"Severity-weighted density\" if weight_by_severity else \"Density\"))\n    plt.title(\"National KDE density\" + (\" — weighted by severity\" if weight_by_severity else \"\"))\n    plt.xlabel(\"Longitude\")\n    plt.ylabel(\"Latitude\")\n    plt.tight_layout()\n    plt.show()\n\n\n\ndef top_states_and_cities(df: pd.DataFrame, top_states: int = 5, top_cities_per_state: int = 5) -> tuple[pd.Series, dict]:\n    \"\"\"\n    Identify top states by accident volume; then top cities inside each.\n    \"\"\"\n    _ensure_cols(df, [STATE, CITY])\n\n    state_counts = df[STATE].value_counts().head(top_states)\n    city_dict = {}\n\n    for st in state_counts.index:\n        sub = df[df[STATE] == st]\n        city_counts = sub[CITY].value_counts().head(top_cities_per_state)\n        city_dict[st] = city_counts\n\n    return state_counts, city_dict\n\n\ndef classify_urban_rural(df: pd.DataFrame, top_city_share: float = 0.80) -> pd.DataFrame:\n    \"\"\"\n    Urban vs Rural proxy (since we don't have population density):\n    For each State, take the smallest set of top cities that cover top_city_share\n    of accidents => label those as Urban; others as Rural.\n\n    This is fast and defensible for storytelling.\n    \"\"\"\n    _ensure_cols(df, [STATE, CITY])\n\n    out = df.copy()\n    out[\"_urban_flag\"] = False\n\n    for st, sub_idx in out.groupby(STATE).groups.items():\n        sub = out.loc[sub_idx, CITY]\n        counts = sub.value_counts()\n        cum_share = counts.cumsum() / counts.sum()\n        urban_cities = cum_share[cum_share <= top_city_share].index.tolist()\n        if not urban_cities:\n            urban_cities = [counts.index[0]]  # at least top city\n        mask = (out[STATE] == st) & (out[CITY].isin(urban_cities))\n        out.loc[mask, \"_urban_flag\"] = True\n\n    out[\"urban_rural\"] = np.where(out[\"_urban_flag\"], \"Urban\", \"Rural\")\n    out = out.drop(columns=[\"_urban_flag\"])\n    return out\n\n\ndef compare_urban_rural_density(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Compare accident intensity in Urban vs Rural zones (proxy).\n    \"\"\"\n    _ensure_cols(df, [\"urban_rural\", SEV])\n\n    tab = (\n        df.groupby(\"urban_rural\")[SEV]\n          .agg(n=\"count\", mean_severity=\"mean\", median_severity=\"median\")\n          .sort_values(\"n\", ascending=False)\n    )\n\n    # Plot\n    plt.figure()\n    plt.bar(tab.index.astype(str), tab[\"n\"].values)\n    plt.title(\"Accident volume: Urban vs Rural (proxy)\")\n    plt.xlabel(\"Zone\")\n    plt.ylabel(\"Accident count\")\n    plt.tight_layout()\n    plt.show()\n\n    plt.figure()\n    plt.bar(tab.index.astype(str), tab[\"mean_severity\"].values)\n    plt.title(\"Mean severity: Urban vs Rural (proxy)\")\n    plt.xlabel(\"Zone\")\n    plt.ylabel(\"Mean severity\")\n    plt.tight_layout()\n    plt.show()\n\n    return tab\n\n\ndef plot_state_hexbin(df: pd.DataFrame, state_code: str, gridsize: int = 55, weight_by_severity: bool = True) -> None:\n    \"\"\"\n    Density map for one state (hexbin).\n    \"\"\"\n    _ensure_cols(df, [STATE, LAT, LNG, SEV])\n\n    sub = df[df[STATE] == state_code].dropna(subset=[LAT, LNG, SEV])\n    if sub.empty:\n        print(f\"No data for state={state_code}\")\n        return\n\n    x = sub[LNG].values\n    y = sub[LAT].values\n    C = _severity_weight(sub[SEV]).values if weight_by_severity else None\n\n    plt.figure()\n    hb = plt.hexbin(\n        x, y,\n        C=C,\n        reduce_C_function=np.sum if weight_by_severity else None,\n        gridsize=gridsize,\n        mincnt=1\n    )\n    plt.colorbar(hb, label=(\"Severity-weighted count\" if weight_by_severity else \"Accident count\"))\n    plt.title(f\"{state_code} accident density (hexbin)\" + (\" — severity-weighted\" if weight_by_severity else \"\"))\n    plt.xlabel(\"Longitude\")\n    plt.ylabel(\"Latitude\")\n    plt.tight_layout()\n    plt.show()\n\ndef build_hotspots(df: pd.DataFrame, grid_deg: float = 0.05, top_n: int = 15, weight_by_severity: bool = True) -> pd.DataFrame:\n    \"\"\"\n    Identify hotspots by gridding lat/lng and ranking cells by intensity.\n    Returns a hotspot table with center coordinates and intensity stats.\n    \"\"\"\n    _ensure_cols(df, [LAT, LNG, SEV, HOUR, WEATHER_GRP] + ROAD_FEATURES)\n\n    tmp = df.dropna(subset=[LAT, LNG]).copy()\n    tmp[\"_sev_w\"] = _severity_weight(tmp[SEV])\n\n    tmp = _make_grid(tmp, grid_deg=grid_deg)\n\n   \n    if weight_by_severity:\n        cell_score = tmp.groupby(\"_cell_id\")[\"_sev_w\"].sum().rename(\"intensity\")\n    else:\n        cell_score = tmp.groupby(\"_cell_id\").size().rename(\"intensity\")\n\n    cell_n = tmp.groupby(\"_cell_id\").size().rename(\"n_accidents\")\n    cell_mean_sev = tmp.groupby(\"_cell_id\")[SEV].mean().rename(\"avg_severity\")\n\n    hotspots = pd.concat([cell_score, cell_n, cell_mean_sev], axis=1).sort_values(\"intensity\", ascending=False).head(top_n)\n\n    coords = tmp.groupby(\"_cell_id\")[[\"_lat_bin\", \"_lng_bin\"]].first()\n    hotspots = hotspots.join(coords, how=\"left\").reset_index().rename(columns={\"_cell_id\": \"hotspot_id\"})\n    hotspots[\"hotspot_lat\"] = hotspots[\"_lat_bin\"] + (grid_deg / 2)\n    hotspots[\"hotspot_lng\"] = hotspots[\"_lng_bin\"] + (grid_deg / 2)\n    hotspots = hotspots.drop(columns=[\"_lat_bin\", \"_lng_bin\"])\n\n    return hotspots\n\n\ndef hotspot_profiles(df: pd.DataFrame, hotspots: pd.DataFrame, grid_deg: float = 0.05) -> pd.DataFrame:\n    \"\"\"\n    For each hotspot cell:\n    - Peak hour\n    - Dominant weather group\n    - Common road features (share True)\n    - Average severity\n    \"\"\"\n    _ensure_cols(df, [LAT, LNG, SEV, HOUR, WEATHER_GRP] + ROAD_FEATURES)\n\n    tmp = df.dropna(subset=[LAT, LNG]).copy()\n    tmp = _make_grid(tmp, grid_deg=grid_deg)\n\n\n    keep_ids = set(hotspots[\"hotspot_id\"])\n    tmp = tmp[tmp[\"_cell_id\"].isin(keep_ids)].copy()\n\n    out_rows = []\n    for hid, sub in tmp.groupby(\"_cell_id\"):\n        peak_hour = sub[HOUR].value_counts().idxmax() if sub[HOUR].notna().any() else np.nan\n        dom_weather = sub[WEATHER_GRP].astype(\"string\").value_counts().idxmax() if sub[WEATHER_GRP].notna().any() else \"Unknown\"\n\n        road_shares = {}\n        for rf in ROAD_FEATURES:\n            road_shares[f\"{rf}_share_true\"] = float(sub[rf].astype(bool).mean())\n\n        row = {\n            \"hotspot_id\": hid,\n            \"n_accidents\": int(len(sub)),\n            \"avg_severity\": float(sub[SEV].mean()),\n            \"peak_hour\": int(peak_hour) if pd.notna(peak_hour) else np.nan,\n            \"dominant_weather\": dom_weather,\n        }\n        row.update(road_shares)\n        out_rows.append(row)\n\n    profiles = pd.DataFrame(out_rows)\n\n    profiles = profiles.merge(hotspots[[\"hotspot_id\", \"intensity\", \"hotspot_lat\", \"hotspot_lng\"]], on=\"hotspot_id\", how=\"left\")\n    profiles = profiles.sort_values(\"intensity\", ascending=False).reset_index(drop=True)\n    return profiles\n\n\ndef plot_hotspots_on_hexbin(df: pd.DataFrame, hotspots: pd.DataFrame, gridsize: int = 70, weight_by_severity: bool = True) -> None:\n    \"\"\"\n    Overlay hotspot centers on top of the national hexbin density map.\n    \"\"\"\n    _ensure_cols(df, [LAT, LNG, SEV])\n\n    tmp = df[[LAT, LNG, SEV]].dropna()\n    x = tmp[LNG].values\n    y = tmp[LAT].values\n    C = _severity_weight(tmp[SEV]).values if weight_by_severity else None\n\n    plt.figure()\n    hb = plt.hexbin(\n        x, y,\n        C=C,\n        reduce_C_function=np.sum if weight_by_severity else None,\n        gridsize=gridsize,\n        mincnt=1\n    )\n    plt.colorbar(hb, label=(\"Severity-weighted count\" if weight_by_severity else \"Accident count\"))\n    plt.scatter(hotspots[\"hotspot_lng\"].values, hotspots[\"hotspot_lat\"].values, s=40)\n    plt.title(\"National density + hotspot centers\")\n    plt.xlabel(\"Longitude\")\n    plt.ylabel(\"Latitude\")\n    plt.tight_layout()\n    plt.show()\n\n\ndef run_geospatial_hotspot_analysis(df_features: pd.DataFrame,\n                                   top_states: int = 5,\n                                   top_cities_per_state: int = 5,\n                                   grid_deg: float = 0.05,\n                                   top_hotspots: int = 15) -> dict:\n    \"\"\"\n    Outputs:\n      - National maps (scatter + hexbin severity-weighted)\n      - Top states + top cities per state\n      - Urban vs Rural comparison (proxy)\n      - Hotspot table + profiles\n      - Hotspot overlay map\n    \"\"\"\n    _ensure_cols(df_features, [LAT, LNG, SEV, STATE, CITY])\n\n    outputs = {}\n\n  \n    print(\"\\n=== National Accident Density Mapping ===\")\n    plot_national_scatter(df_features)\n    plot_national_hexbin(df_features, gridsize=70, weight_by_severity=True)\n\n    plot_national_kde(df_features, sample_n=15000, weight_by_severity=True)\n\n    print(\"\\n===  State & City-Level Hotspots ===\")\n    state_counts, city_dict = top_states_and_cities(df_features, top_states=top_states, top_cities_per_state=top_cities_per_state)\n    outputs[\"top_states\"] = state_counts\n    outputs[\"top_cities_per_state\"] = city_dict\n\n    print(\"\\nTop states by accident volume:\")\n    print(state_counts)\n\n    print(\"\\nTop cities inside each top state:\")\n    for st, cc in city_dict.items():\n        print(f\"\\nState {st}:\")\n        print(cc)\n\n \n    for st in state_counts.index[:2]:\n        plot_state_hexbin(df_features, state_code=st, gridsize=55, weight_by_severity=True)\n\n   \n    df_ur = classify_urban_rural(df_features, top_city_share=0.80)\n    outputs[\"urban_rural_table\"] = compare_urban_rural_density(df_ur)\n\n \n    print(\"\\n=== Hotspot Profiling ===\")\n    hotspots = build_hotspots(df_ur, grid_deg=grid_deg, top_n=top_hotspots, weight_by_severity=True)\n    profiles = hotspot_profiles(df_ur, hotspots, grid_deg=grid_deg)\n\n    outputs[\"hotspots\"] = hotspots\n    outputs[\"hotspot_profiles\"] = profiles\n\n    print(\"\\nHotspots (top cells):\")\n    print(hotspots)\n\n    print(\"\\nHotspot profiles:\")\n    print(profiles)\n\n    plot_hotspots_on_hexbin(df_ur, hotspots, gridsize=70, weight_by_severity=True)\n\n    return outputs\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T10:06:34.413511Z","iopub.execute_input":"2025-12-30T10:06:34.413980Z","iopub.status.idle":"2025-12-30T10:06:34.474217Z","shell.execute_reply.started":"2025-12-30T10:06:34.413930Z","shell.execute_reply":"2025-12-30T10:06:34.473165Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## CONTRIBUTING FACTOR ANALYSIS","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\ntry:\n    from scipy.stats import chi2_contingency, kruskal, f_oneway\n    _HAS_SCIPY = True\nexcept Exception:\n    _HAS_SCIPY = False\n\ntry:\n    from sklearn.model_selection import StratifiedKFold, cross_val_score\n    from sklearn.preprocessing import OneHotEncoder\n    from sklearn.compose import ColumnTransformer\n    from sklearn.pipeline import Pipeline\n    from sklearn.impute import SimpleImputer\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.metrics import roc_auc_score, average_precision_score\n    from sklearn.inspection import permutation_importance\n    _HAS_SKLEARN = True\nexcept Exception:\n    _HAS_SKLEARN = False\n\ntry:\n    import shap\n    _HAS_SHAP = True\nexcept Exception:\n    _HAS_SHAP = False\n\n\n\nSEV = \"Severity\"\nWEATHER_GRP = \"weather_group\"\nVIS = \"Visibility(mi)\"\nWIND = \"Wind_Speed(mph)\"\nTEMP = \"Temperature(F)\"\n\nROAD_FEATURES = [\n    \"Junction\", \"Traffic_Signal\", \"Crossing\", \"Stop\", \"Give_Way\",\n    \"Traffic_Calming\", \"No_Exit\", \"Roundabout\", \"Railway\", \"Station\", \"Amenity\"\n]\n\nTIME_FEATURES = [\"start_hour\", \"is_weekend\", \"season\"]\nOTHER_NUM = [\"accident_duration_min\", \"Distance(mi)\"]\n\n\n\ndef cramers_v(contingency: np.ndarray) -> float:\n    \"\"\"\n    Cramér’s V effect size for Chi-square association.\n    \"\"\"\n    if contingency.size == 0:\n        return np.nan\n    chi2, _, _, _ = chi2_contingency(contingency, correction=False)\n    n = contingency.sum()\n    if n == 0:\n        return np.nan\n    r, k = contingency.shape\n    phi2 = chi2 / n\n    denom = min(k - 1, r - 1)\n    return np.sqrt(phi2 / denom) if denom > 0 else np.nan\n\n\ndef eta_squared_anova(groups: list[np.ndarray]) -> float:\n    \"\"\"\n    Eta-squared effect size for one-way ANOVA:\n      η² = SSB / SST\n    \"\"\"\n    # flatten\n    groups = [g[~np.isnan(g)] for g in groups if len(g) > 0]\n    if len(groups) < 2:\n        return np.nan\n    all_vals = np.concatenate(groups)\n    if len(all_vals) == 0:\n        return np.nan\n\n    overall_mean = np.mean(all_vals)\n    ssb = sum(len(g) * (np.mean(g) - overall_mean) ** 2 for g in groups)\n    sst = sum((all_vals - overall_mean) ** 2)\n    return ssb / sst if sst > 0 else np.nan\n\n\ndef epsilon_squared_kruskal(h_stat: float, n: int, k: int) -> float:\n    \"\"\"\n    Epsilon-squared effect size for Kruskal-Wallis:\n      ε² = (H - k + 1) / (n - k)\n    \"\"\"\n    denom = (n - k)\n    if denom <= 0:\n        return np.nan\n    return (h_stat - k + 1) / denom\n\n\ndef chi_square_association(df: pd.DataFrame, cat_col: str, target_col: str = SEV) -> dict:\n    \"\"\"\n    Severity vs categorical variable using Chi-square + Cramér's V.\n    \"\"\"\n    tab = pd.crosstab(df[cat_col], df[target_col])\n    if tab.empty or not _HAS_SCIPY:\n        return {\n            \"feature\": cat_col, \"test\": \"chi2\",\n            \"p_value\": np.nan, \"effect_size\": np.nan,\n            \"effect_name\": \"CramersV\", \"notes\": \"SciPy missing or empty table\"\n        }\n\n    chi2, p, dof, _ = chi2_contingency(tab.values, correction=False)\n    v = cramers_v(tab.values)\n\n    return {\n        \"feature\": cat_col,\n        \"test\": \"chi2\",\n        \"p_value\": float(p),\n        \"effect_size\": float(v),\n        \"effect_name\": \"CramersV\",\n        \"dof\": int(dof),\n        \"n\": int(tab.values.sum())\n    }\n\n\ndef numeric_vs_severity(df: pd.DataFrame, num_col: str, target_col: str = SEV, method: str = \"auto\") -> dict:\n    \"\"\"\n    Severity vs numeric variable using ANOVA (param) or Kruskal (non-param).\n    Reports effect size: eta^2 (ANOVA) or epsilon^2 (Kruskal).\n    \"\"\"\n    x = pd.to_numeric(df[num_col], errors=\"coerce\")\n    y = df[target_col]\n\n    valid = x.notna() & y.notna()\n    x = x[valid]\n    y = y[valid]\n\n    if len(x) < 30 or y.nunique() < 2 or not _HAS_SCIPY:\n        return {\n            \"feature\": num_col, \"test\": \"anova/kruskal\",\n            \"p_value\": np.nan, \"effect_size\": np.nan,\n            \"effect_name\": \"eta2/epsilon2\", \"notes\": \"SciPy missing or insufficient data\"\n        }\n\n    # build groups by severity\n    groups = [x[y == s].values for s in sorted(y.unique())]\n    k = len(groups)\n    n = len(x)\n\n    # Choose method\n    if method == \"anova\":\n        stat, p = f_oneway(*groups)\n        eff = eta_squared_anova(groups)\n        return {\n            \"feature\": num_col, \"test\": \"anova\",\n            \"p_value\": float(p),\n            \"effect_size\": float(eff),\n            \"effect_name\": \"eta2\",\n            \"n\": int(n),\n            \"k_groups\": int(k),\n        }\n\n    if method == \"kruskal\":\n        stat, p = kruskal(*groups)\n        eff = epsilon_squared_kruskal(float(stat), n=n, k=k)\n        return {\n            \"feature\": num_col, \"test\": \"kruskal\",\n            \"p_value\": float(p),\n            \"effect_size\": float(eff),\n            \"effect_name\": \"epsilon2\",\n            \"n\": int(n),\n            \"k_groups\": int(k),\n        }\n\n    stat, p = kruskal(*groups)\n    eff = epsilon_squared_kruskal(float(stat), n=n, k=k)\n    return {\n        \"feature\": num_col, \"test\": \"kruskal(auto)\",\n        \"p_value\": float(p),\n        \"effect_size\": float(eff),\n        \"effect_name\": \"epsilon2\",\n        \"n\": int(n),\n        \"k_groups\": int(k),\n    }\n\n\ndef statistical_contributing_factors(df_features: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Returns ranked table of contributing factors using:\n      - Chi-square + Cramér's V for categorical/binary\n      - Kruskal + epsilon^2 for numeric vs severity\n    Ranking: primarily by effect size (descending), then p-value.\n    \"\"\"\n    results = []\n\n    # --- Severity vs weather group (categorical)\n    if WEATHER_GRP in df_features.columns:\n        results.append(chi_square_association(df_features.dropna(subset=[WEATHER_GRP, SEV]), WEATHER_GRP))\n\n    # --- Severity vs road conditions (binary)\n    for c in ROAD_FEATURES:\n        if c in df_features.columns:\n            results.append(chi_square_association(df_features.dropna(subset=[c, SEV]), c))\n\n    # --- Severity vs numeric weather\n    for c in [VIS, WIND, TEMP]:\n        if c in df_features.columns:\n            results.append(numeric_vs_severity(df_features, c, method=\"auto\"))\n\n    # --- Optional: time + exposure numeric\n    for c in OTHER_NUM:\n        if c in df_features.columns:\n            results.append(numeric_vs_severity(df_features, c, method=\"auto\"))\n\n    # --- Time categorical (season)\n    if \"season\" in df_features.columns:\n        results.append(chi_square_association(df_features.dropna(subset=[\"season\", SEV]), \"season\"))\n    # Weekend flag (binary)\n    if \"is_weekend\" in df_features.columns:\n        results.append(chi_square_association(df_features.dropna(subset=[\"is_weekend\", SEV]), \"is_weekend\"))\n\n    res = pd.DataFrame(results)\n\n    # Normalize effect sizes for fair ranking across different metrics\n    # (Cramér’s V ∈ [0,1]; epsilon² roughly [0,1] often small; both are \"higher = stronger\")\n    res[\"effect_size\"] = pd.to_numeric(res[\"effect_size\"], errors=\"coerce\")\n    res[\"p_value\"] = pd.to_numeric(res[\"p_value\"], errors=\"coerce\")\n\n    res = res.sort_values(\n        by=[\"effect_size\", \"p_value\"],\n        ascending=[False, True],\n        na_position=\"last\"\n    ).reset_index(drop=True)\n\n    return res\n\n\n# ============================================================\n# 5.2 Predictive Insight (Optional): Logistic Regression + Importance + SHAP\n# ============================================================\n\ndef build_binary_target(df: pd.DataFrame, sev_col: str = SEV) -> pd.Series:\n    \"\"\"\n    Low severity: 1–2 -> 0\n    High severity: 3–4 -> 1\n    \"\"\"\n    sev = pd.to_numeric(df[sev_col], errors=\"coerce\")\n    y = pd.Series(np.nan, index=df.index, dtype=\"float32\")\n    y.loc[sev.isin([1, 2])] = 0\n    y.loc[sev.isin([3, 4])] = 1\n    return y.astype(\"Int8\")\n\n\ndef train_interpretable_model(df_features: pd.DataFrame,\n                              use_tree: bool = False,\n                              random_state: int = 42) -> dict:\n    \"\"\"\n    Trains an interpretable classifier for High vs Low severity.\n    Default: Logistic Regression (strong baseline + explainable).\n    Also computes permutation importance.\n    SHAP explanation is attempted if shap is installed.\n\n    Returns dict with:\n      - pipeline\n      - cv_auc, cv_ap\n      - permutation_importance_df\n      - shap_values (optional)\n    \"\"\"\n    if not _HAS_SKLEARN:\n        raise ImportError(\"scikit-learn is required for predictive insight section.\")\n\n    df = df_features.copy()\n\n    # Binary target\n    y = build_binary_target(df, SEV)\n    mask = y.notna()\n    df = df.loc[mask].copy()\n    y = y.loc[mask].astype(int)\n\n    # Candidate features (tight & explainable)\n    cat_cols = [c for c in [\"weather_group\", \"season\", \"day_name\"] if c in df.columns]\n    num_cols = [c for c in [VIS, WIND, TEMP, \"start_hour\", \"accident_duration_min\", \"Distance(mi)\"] if c in df.columns]\n    bin_cols = [c for c in ROAD_FEATURES + [\"is_weekend\", \"traffic_control_presence\", \"urban_infra_proximity\"] if c in df.columns]\n\n    # Keep only existing columns\n    X = df[cat_cols + num_cols + bin_cols].copy()\n\n    # Preprocess\n    numeric_tf = Pipeline(steps=[\n        (\"imputer\", SimpleImputer(strategy=\"median\")),\n    ])\n\n    categorical_tf = Pipeline(steps=[\n        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)),\n    ])\n\n    # For binary columns: impute + pass through\n    binary_tf = Pipeline(steps=[\n        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    ])\n\n    pre = ColumnTransformer(\n        transformers=[\n            (\"num\", numeric_tf, num_cols),\n            (\"cat\", categorical_tf, cat_cols),\n            (\"bin\", binary_tf, bin_cols),\n        ],\n        remainder=\"drop\"\n    )\n\n    # Model choices\n    if use_tree:\n        from sklearn.tree import DecisionTreeClassifier\n        model = DecisionTreeClassifier(\n            max_depth=4,\n            min_samples_leaf=200,\n            random_state=random_state\n        )\n        model_name = \"DecisionTree(depth<=4)\"\n    else:\n        model = LogisticRegression(\n            max_iter=2000,\n            class_weight=\"balanced\",\n            solver=\"liblinear\",\n            random_state=random_state\n        )\n        model_name = \"LogisticRegression(balanced)\"\n\n    pipe = Pipeline(steps=[(\"pre\", pre), (\"model\", model)])\n\n    # CV evaluation (AUC + Average Precision)\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n    auc = cross_val_score(pipe, X, y, cv=cv, scoring=\"roc_auc\").mean()\n    ap = cross_val_score(pipe, X, y, cv=cv, scoring=\"average_precision\").mean()\n\n    # Fit final\n    pipe.fit(X, y)\n\n    # Permutation importance (model-agnostic, stable, interpretable)\n    perm = permutation_importance(\n        pipe, X, y,\n        n_repeats=15,\n        random_state=random_state,\n        scoring=\"roc_auc\"\n    )\n\n    # Get feature names (after preprocessing)\n    feature_names = []\n    # numeric\n    feature_names.extend(num_cols)\n    # categorical onehot\n    if cat_cols:\n        ohe = pipe.named_steps[\"pre\"].named_transformers_[\"cat\"].named_steps[\"onehot\"]\n        cat_ohe_names = ohe.get_feature_names_out(cat_cols).tolist()\n        feature_names.extend(cat_ohe_names)\n    # binary\n    feature_names.extend(bin_cols)\n\n    perm_df = pd.DataFrame({\n        \"feature\": feature_names,\n        \"importance_mean\": perm.importances_mean,\n        \"importance_std\": perm.importances_std\n    }).sort_values(\"importance_mean\", ascending=False).reset_index(drop=True)\n\n    out = {\n        \"model_name\": model_name,\n        \"pipeline\": pipe,\n        \"cv_auc\": float(auc),\n        \"cv_ap\": float(ap),\n        \"permutation_importance\": perm_df\n    }\n\n    # SHAP (optional)\n    if _HAS_SHAP:\n        try:\n            # For logistic regression, use LinearExplainer on transformed matrix\n            X_trans = pipe.named_steps[\"pre\"].transform(X)\n            # Need the underlying model\n            mdl = pipe.named_steps[\"model\"]\n            explainer = shap.Explainer(mdl, X_trans)\n            shap_values = explainer(X_trans)\n            out[\"shap_values\"] = shap_values\n            out[\"shap_feature_names\"] = feature_names\n        except Exception as e:\n            out[\"shap_error\"] = str(e)\n\n    return out\n\n\n# ============================================================\n# Master: ranked contributing factors (stats + optional ML)\n# ============================================================\ndef contributing_factor_analysis(df_features: pd.DataFrame, run_predictive: bool = True) -> dict:\n    \"\"\"\n    Returns:\n      - stats_ranked_factors: DataFrame (effect sizes + p-values)\n      - (optional) predictive: model results with permutation importance (+ SHAP if available)\n    \"\"\"\n    print(\"\\n=== 5.1 Statistical Association Analysis (with effect sizes) ===\")\n    stats_ranked = statistical_contributing_factors(df_features)\n    print(stats_ranked.head(20))\n\n    outputs = {\"stats_ranked_factors\": stats_ranked}\n\n    if run_predictive:\n        print(\"\\n=== 5.2 Predictive Insight (interpretable model) ===\")\n        pred = train_interpretable_model(df_features, use_tree=False)\n        print(f\"Model: {pred['model_name']}\")\n        print(f\"CV ROC-AUC: {pred['cv_auc']:.4f}\")\n        print(f\"CV Avg Precision: {pred['cv_ap']:.4f}\")\n        print(\"\\nTop permutation importances:\")\n        print(pred[\"permutation_importance\"].head(20))\n        outputs[\"predictive\"] = pred\n\n    return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T10:06:34.475638Z","iopub.execute_input":"2025-12-30T10:06:34.475939Z","iopub.status.idle":"2025-12-30T10:06:39.240135Z","shell.execute_reply.started":"2025-12-30T10:06:34.475904Z","shell.execute_reply":"2025-12-30T10:06:39.239079Z"}},"outputs":[],"execution_count":10}]}